{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "169bb2b6",
   "metadata": {},
   "source": [
    "# Evaluating Fraud Detection Techniques in Banking and Insurance Using Data Science\n",
    "\n",
    "## Comprehensive Analysis Using Machine Learning Approaches\n",
    "\n",
    "**Author:** [Your Name]  \n",
    "**Institution:** [Your Institution]  \n",
    "**Date:** August 2025  \n",
    "**Dataset:** Credit Card Fraud Detection Dataset (Kaggle MLG-ULB)\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Introduction and Literature Review](#introduction)\n",
    "2. [Data Loading and Exploration](#data-loading)\n",
    "3. [Data Preprocessing and Feature Engineering](#preprocessing)\n",
    "4. [Model Implementation](#models)\n",
    "5. [Model Evaluation and Comparison](#evaluation)\n",
    "6. [Hyperparameter Tuning](#tuning)\n",
    "7. [Ethical Considerations](#ethics)\n",
    "8. [Conclusions](#conclusions)\n",
    "9. [References](#references)\n",
    "\n",
    "---\n",
    "\n",
    "<a id=\"introduction\"></a>\n",
    "\n",
    "## 1. Introduction and Literature Review\n",
    "\n",
    "### Problem Statement\n",
    "\n",
    "Financial fraud detection represents a critical challenge in modern banking and insurance sectors, with global fraud losses reaching billions annually. This analysis evaluates multiple machine learning approaches for detecting fraudulent credit card transactions, contributing to the growing body of research on automated fraud detection systems.\n",
    "\n",
    "### Research Objectives\n",
    "\n",
    "1. Compare the performance of traditional and ensemble machine learning algorithms for fraud detection\n",
    "2. Evaluate feature engineering techniques for imbalanced financial datasets\n",
    "3. Assess model interpretability and ethical implications in fraud detection systems\n",
    "4. Provide recommendations for practical implementation in banking environments\n",
    "\n",
    "### Literature Context\n",
    "\n",
    "Recent systematic reviews highlight the effectiveness of ensemble methods and neural networks in fraud detection (Zareapoor et al., 2024). Support Vector Machines and Artificial Neural Networks have emerged as particularly effective approaches for credit card fraud detection (Ahmad et al., 2022). The challenge of class imbalance in fraud datasets has driven research toward advanced sampling techniques and cost-sensitive learning approaches (Borketey, 2024).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb5e44f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xgboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 19\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpipeline\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Pipeline\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m resample\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mxgboost\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mxgb\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mimblearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mover_sampling\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SMOTE\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mimblearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01munder_sampling\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RandomUnderSampler\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'xgboost'"
     ]
    }
   ],
   "source": [
    "# Cell 1: Import Required Libraries and Setup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Machine Learning Libraries\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import (classification_report, confusion_matrix, roc_auc_score, \n",
    "                           roc_curve, precision_recall_curve, f1_score, accuracy_score,\n",
    "                           precision_score, recall_score)\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.utils import resample\n",
    "import xgboost as xgb\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "# Deep Learning Libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# Statistical Libraries\n",
    "from scipy import stats\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('default')  # Use default style instead of deprecated seaborn-v0_8\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"XGBoost version: {xgb.__version__}\")\n",
    "print(f\"Scikit-learn version: {sklearn.__version__}\")\n",
    "print(\"All required packages are now available!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8cd520f",
   "metadata": {},
   "source": [
    "<a id=\"data-loading\"></a>\n",
    "\n",
    "## 2. Data Loading and Exploration\n",
    "\n",
    "**Dataset Information:**\n",
    "\n",
    "- Load the Credit Card Fraud Detection dataset from Kaggle\n",
    "- Dataset source: https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud\n",
    "- Note: This dataset contains transactions made by credit cards in September 2013 by European cardholders\n",
    "- Features V1-V28 are the result of PCA transformation to protect user privacy\n",
    "\n",
    "The following analysis will load the dataset and perform comprehensive exploratory data analysis to understand the structure, distribution, and characteristics of the fraud detection dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8ffee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Data Loading and Initial Exploration\n",
    "\n",
    "# Load the dataset\n",
    "# Note: Adjust the path according to your local setup\n",
    "try:\n",
    "    df = pd.read_csv('creditcard.csv')\n",
    "    print(\"Dataset loaded successfully!\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Dataset not found. Please ensure 'creditcard.csv' is in your working directory.\")\n",
    "    print(\"Download from: https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud\")\n",
    "    # Create a sample dataset for demonstration purposes\n",
    "    np.random.seed(42)\n",
    "    n_samples = 10000\n",
    "    n_features = 30\n",
    "    \n",
    "    # Generate synthetic data similar to the original dataset\n",
    "    data = np.random.randn(n_samples, n_features)\n",
    "    \n",
    "    # Create realistic 'Time' and 'Amount' features\n",
    "    time_data = np.random.uniform(0, 172800, n_samples)  # 48 hours in seconds\n",
    "    amount_data = np.random.lognormal(3, 1.5, n_samples)  # Log-normal distribution for amounts\n",
    "    \n",
    "    # Create highly imbalanced target (0.17% fraud rate similar to original)\n",
    "    fraud_indices = np.random.choice(n_samples, size=int(0.0017 * n_samples), replace=False)\n",
    "    target = np.zeros(n_samples)\n",
    "    target[fraud_indices] = 1\n",
    "    \n",
    "    # Combine features\n",
    "    feature_columns = [f'V{i}' for i in range(1, 29)]\n",
    "    df = pd.DataFrame(data[:, :28], columns=feature_columns)\n",
    "    df['Time'] = time_data\n",
    "    df['Amount'] = amount_data\n",
    "    df['Class'] = target.astype(int)\n",
    "    \n",
    "    print(\"Using synthetic dataset for demonstration purposes.\")\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(f\"\\nDataset Shape: {df.shape}\")\n",
    "print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "print(\"\\nColumn names:\")\n",
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9752b53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Basic Dataset Information and Statistical Summary\n",
    "\n",
    "# Basic dataset information\n",
    "print(\"=== DATASET OVERVIEW ===\")\n",
    "print(f\"Dataset dimensions: {df.shape[0]} rows Ã— {df.shape[1]} columns\")\n",
    "print(f\"Total features: {len(df.columns) - 1}\")\n",
    "print(f\"Target variable: Class (0: Normal, 1: Fraud)\")\n",
    "\n",
    "# Check for missing values\n",
    "print(f\"\\nMissing values per column:\")\n",
    "missing_values = df.isnull().sum()\n",
    "if missing_values.sum() == 0:\n",
    "    print(\"No missing values detected - excellent data quality!\")\n",
    "else:\n",
    "    print(missing_values[missing_values > 0])\n",
    "\n",
    "# Target variable distribution\n",
    "fraud_count = df['Class'].value_counts()\n",
    "fraud_percentage = df['Class'].value_counts(normalize=True) * 100\n",
    "\n",
    "print(f\"\\n=== CLASS DISTRIBUTION ===\")\n",
    "print(f\"Normal transactions: {fraud_count[0]:,} ({fraud_percentage[0]:.3f}%)\")\n",
    "print(f\"Fraudulent transactions: {fraud_count[1]:,} ({fraud_percentage[1]:.3f}%)\")\n",
    "print(f\"Imbalance ratio: {fraud_count[0]/fraud_count[1]:.1f}:1\")\n",
    "\n",
    "# Statistical summary\n",
    "print(f\"\\n=== STATISTICAL SUMMARY ===\")\n",
    "print(df.describe())\n",
    "\n",
    "# Data types\n",
    "print(f\"\\n=== DATA TYPES ===\")\n",
    "print(df.dtypes.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a89150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Advanced Exploratory Data Analysis with Visualizations\n",
    "\n",
    "# Create comprehensive visualizations to understand data distribution,\n",
    "# feature relationships, and fraud patterns.\n",
    "\n",
    "# Create figure with multiple subplots\n",
    "fig = plt.figure(figsize=(16, 12))\n",
    "\n",
    "# 1. Class distribution\n",
    "plt.subplot(2, 3, 1)\n",
    "fraud_counts = df['Class'].value_counts()\n",
    "colors = ['#2ecc71', '#e74c3c']\n",
    "plt.pie(fraud_counts.values, labels=['Normal', 'Fraud'], autopct='%1.2f%%', \n",
    "        colors=colors, startangle=90)\n",
    "plt.title('Transaction Distribution', fontsize=14, fontweight='bold')\n",
    "\n",
    "# 2. Amount distribution by class\n",
    "plt.subplot(2, 3, 2)\n",
    "normal_amounts = df[df['Class'] == 0]['Amount']\n",
    "fraud_amounts = df[df['Class'] == 1]['Amount']\n",
    "\n",
    "plt.hist(normal_amounts, bins=50, alpha=0.7, label='Normal', color='#2ecc71', density=True)\n",
    "plt.hist(fraud_amounts, bins=50, alpha=0.7, label='Fraud', color='#e74c3c', density=True)\n",
    "plt.xlabel('Transaction Amount')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Amount Distribution by Class')\n",
    "plt.legend()\n",
    "plt.yscale('log')\n",
    "\n",
    "# 3. Time distribution analysis\n",
    "plt.subplot(2, 3, 3)\n",
    "plt.hist(df['Time'], bins=50, alpha=0.7, color='#3498db')\n",
    "plt.xlabel('Time (seconds)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Transaction Time Distribution')\n",
    "\n",
    "# 4. Amount vs Time scatter plot\n",
    "plt.subplot(2, 3, 4)\n",
    "normal_data = df[df['Class'] == 0].sample(n=min(5000, len(df[df['Class'] == 0])))\n",
    "fraud_data = df[df['Class'] == 1]\n",
    "\n",
    "plt.scatter(normal_data['Time'], normal_data['Amount'], alpha=0.5, \n",
    "           label='Normal', color='#2ecc71', s=1)\n",
    "plt.scatter(fraud_data['Time'], fraud_data['Amount'], alpha=0.8, \n",
    "           label='Fraud', color='#e74c3c', s=10)\n",
    "plt.xlabel('Time (seconds)')\n",
    "plt.ylabel('Amount')\n",
    "plt.title('Amount vs Time by Class')\n",
    "plt.legend()\n",
    "plt.yscale('log')\n",
    "\n",
    "# 5. Correlation heatmap for key features\n",
    "plt.subplot(2, 3, 5)\n",
    "# Select a subset of features for correlation analysis\n",
    "key_features = ['Time', 'Amount'] + [f'V{i}' for i in range(1, 11)] + ['Class']\n",
    "corr_matrix = df[key_features].corr()\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "sns.heatmap(corr_matrix, mask=mask, annot=False, cmap='coolwarm', center=0,\n",
    "            square=True, linewidths=0.5)\n",
    "plt.title('Feature Correlation Matrix')\n",
    "\n",
    "# 6. Box plot for amount by class\n",
    "plt.subplot(2, 3, 6)\n",
    "df_plot = df.copy()\n",
    "df_plot['Class_Label'] = df_plot['Class'].map({0: 'Normal', 1: 'Fraud'})\n",
    "sns.boxplot(data=df_plot, x='Class_Label', y='Amount')\n",
    "plt.title('Amount Distribution by Class')\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print statistical insights\n",
    "print(\"=== KEY INSIGHTS FROM EDA ===\")\n",
    "print(f\"Average transaction amount (Normal): ${normal_amounts.mean():.2f}\")\n",
    "print(f\"Average transaction amount (Fraud): ${fraud_amounts.mean():.2f}\")\n",
    "print(f\"Median transaction amount (Normal): ${normal_amounts.median():.2f}\")\n",
    "print(f\"Median transaction amount (Fraud): ${fraud_amounts.median():.2f}\")\n",
    "print(f\"Maximum transaction amount: ${df['Amount'].max():.2f}\")\n",
    "print(f\"Time span: {df['Time'].max() - df['Time'].min():.0f} seconds ({(df['Time'].max() - df['Time'].min())/3600:.1f} hours)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d173a6e",
   "metadata": {},
   "source": [
    "<a id=\"preprocessing\"></a>\n",
    "\n",
    "## 3. Data Preprocessing and Feature Engineering\n",
    "\n",
    "This section implements comprehensive preprocessing including:\n",
    "\n",
    "- Feature scaling and normalization\n",
    "- Handling class imbalance using multiple techniques\n",
    "- Feature selection and engineering\n",
    "- Data splitting with stratification\n",
    "\n",
    "The preprocessing pipeline is designed to optimize model performance while maintaining data integrity and addressing the significant class imbalance present in fraud detection datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a32f5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Feature Engineering and Data Preprocessing\n",
    "\n",
    "class FraudDetectionPreprocessor:\n",
    "    \"\"\"\n",
    "    Custom preprocessing class for fraud detection with comprehensive\n",
    "    feature engineering and data preparation capabilities.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, scaling_method='robust'):\n",
    "        self.scaling_method = scaling_method\n",
    "        self.scaler = None\n",
    "        self.feature_names = None\n",
    "        \n",
    "    def create_time_features(self, df):\n",
    "        \"\"\"Extract time-based features from the Time column\"\"\"\n",
    "        df_processed = df.copy()\n",
    "        \n",
    "        # Convert time to hours (assuming Time is in seconds)\n",
    "        df_processed['Time_Hour'] = (df_processed['Time'] % (24 * 3600)) / 3600\n",
    "        \n",
    "        # Cyclical encoding for hour\n",
    "        df_processed['Time_Hour_Sin'] = np.sin(2 * np.pi * df_processed['Time_Hour'] / 24)\n",
    "        df_processed['Time_Hour_Cos'] = np.cos(2 * np.pi * df_processed['Time_Hour'] / 24)\n",
    "        \n",
    "        return df_processed\n",
    "    \n",
    "    def create_amount_features(self, df):\n",
    "        \"\"\"Engineer features from the Amount column\"\"\"\n",
    "        df_processed = df.copy()\n",
    "        \n",
    "        # Log transformation for amount (add 1 to handle zero values)\n",
    "        df_processed['Amount_Log'] = np.log1p(df_processed['Amount'])\n",
    "        \n",
    "        # Z-score for amount (outlier detection)\n",
    "        df_processed['Amount_Zscore'] = stats.zscore(df_processed['Amount'])\n",
    "        \n",
    "        # Boolean indicators\n",
    "        df_processed['Is_High_Amount'] = (df_processed['Amount'] > df_processed['Amount'].quantile(0.95)).astype(int)\n",
    "        \n",
    "        return df_processed\n",
    "    \n",
    "    def create_pca_features(self, df):\n",
    "        \"\"\"Create additional features from PCA components\"\"\"\n",
    "        df_processed = df.copy()\n",
    "        \n",
    "        # Select V features (PCA components)\n",
    "        v_features = [col for col in df.columns if col.startswith('V')]\n",
    "        \n",
    "        # Create aggregate features\n",
    "        df_processed['V_Sum'] = df_processed[v_features].sum(axis=1)\n",
    "        df_processed['V_Mean'] = df_processed[v_features].mean(axis=1)\n",
    "        df_processed['V_Std'] = df_processed[v_features].std(axis=1)\n",
    "        df_processed['V_Max'] = df_processed[v_features].max(axis=1)\n",
    "        df_processed['V_Min'] = df_processed[v_features].min(axis=1)\n",
    "        df_processed['V_Range'] = df_processed['V_Max'] - df_processed['V_Min']\n",
    "        \n",
    "        # Count of extreme values\n",
    "        threshold = 3\n",
    "        df_processed['V_Extreme_Count'] = (np.abs(df_processed[v_features]) > threshold).sum(axis=1)\n",
    "        \n",
    "        return df_processed\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        \"\"\"Fit the preprocessor and transform the data\"\"\"\n",
    "        # Feature engineering\n",
    "        X_processed = self.create_time_features(X)\n",
    "        X_processed = self.create_amount_features(X_processed)\n",
    "        X_processed = self.create_pca_features(X_processed)\n",
    "        \n",
    "        # Select numerical features for scaling\n",
    "        numerical_features = X_processed.select_dtypes(include=[np.number]).columns\n",
    "        numerical_features = [col for col in numerical_features if col != 'Class']\n",
    "        \n",
    "        # Initialize and fit scaler\n",
    "        if self.scaling_method == 'robust':\n",
    "            self.scaler = RobustScaler()\n",
    "        else:\n",
    "            self.scaler = StandardScaler()\n",
    "        \n",
    "        X_scaled = X_processed[numerical_features].copy()\n",
    "        X_scaled[numerical_features] = self.scaler.fit_transform(X_scaled[numerical_features])\n",
    "        \n",
    "        self.feature_names = numerical_features\n",
    "        \n",
    "        return X_scaled\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"Transform new data using fitted preprocessor\"\"\"\n",
    "        # Feature engineering\n",
    "        X_processed = self.create_time_features(X)\n",
    "        X_processed = self.create_amount_features(X_processed)\n",
    "        X_processed = self.create_pca_features(X_processed)\n",
    "        \n",
    "        # Scale features\n",
    "        X_scaled = X_processed[self.feature_names].copy()\n",
    "        X_scaled[self.feature_names] = self.scaler.transform(X_scaled[self.feature_names])\n",
    "        \n",
    "        return X_scaled\n",
    "\n",
    "# Apply preprocessing\n",
    "print(\"=== FEATURE ENGINEERING AND PREPROCESSING ===\")\n",
    "\n",
    "# Initialize preprocessor\n",
    "preprocessor = FraudDetectionPreprocessor(scaling_method='robust')\n",
    "\n",
    "# Separate features and target\n",
    "X = df.drop('Class', axis=1)\n",
    "y = df['Class']\n",
    "\n",
    "# Apply preprocessing\n",
    "X_processed = preprocessor.fit_transform(X)\n",
    "\n",
    "print(f\"Original features: {X.shape[1]}\")\n",
    "print(f\"Engineered features: {X_processed.shape[1]}\")\n",
    "print(f\"Feature names: {list(X_processed.columns)}\")\n",
    "\n",
    "# Display feature engineering results\n",
    "print(f\"\\nNew features created:\")\n",
    "new_features = [col for col in X_processed.columns if col not in X.columns]\n",
    "print(new_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a71761f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Data Splitting and Class Imbalance Handling\n",
    "\n",
    "# Stratified train-test split to maintain class distribution\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_processed, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"=== DATA SPLITTING RESULTS ===\")\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "print(f\"Training fraud rate: {y_train.mean():.4f}\")\n",
    "print(f\"Test fraud rate: {y_test.mean():.4f}\")\n",
    "\n",
    "# Class imbalance handling strategies\n",
    "print(f\"\\n=== CLASS IMBALANCE HANDLING ===\")\n",
    "\n",
    "# 1. SMOTE (Synthetic Minority Oversampling Technique)\n",
    "smote = SMOTE(random_state=42, k_neighbors=5)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "print(f\"Original training set shape: {X_train.shape}\")\n",
    "print(f\"SMOTE resampled shape: {X_train_smote.shape}\")\n",
    "print(f\"Original fraud rate: {y_train.mean():.4f}\")\n",
    "print(f\"SMOTE fraud rate: {y_train_smote.mean():.4f}\")\n",
    "\n",
    "# 2. Random Undersampling\n",
    "undersampler = RandomUnderSampler(random_state=42, sampling_strategy=0.5)\n",
    "X_train_under, y_train_under = undersampler.fit_resample(X_train, y_train)\n",
    "\n",
    "print(f\"Undersampled shape: {X_train_under.shape}\")\n",
    "print(f\"Undersampled fraud rate: {y_train_under.mean():.4f}\")\n",
    "\n",
    "# Visualization of class distributions\n",
    "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "\n",
    "datasets = [\n",
    "    (y_train, \"Original Training\", '#3498db'),\n",
    "    (y_train_smote, \"SMOTE Resampled\", '#2ecc71'),\n",
    "    (y_train_under, \"Undersampled\", '#e74c3c'),\n",
    "    (y_test, \"Test Set\", '#9b59b6')\n",
    "]\n",
    "\n",
    "for idx, (y_data, title, color) in enumerate(datasets):\n",
    "    counts = pd.Series(y_data).value_counts()\n",
    "    axes[idx].pie(counts.values, labels=['Normal', 'Fraud'], autopct='%1.1f%%',\n",
    "                  colors=['lightblue', color], startangle=90)\n",
    "    axes[idx].set_title(title)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Store different training sets for model comparison\n",
    "training_sets = {\n",
    "    'original': (X_train, y_train),\n",
    "    'smote': (X_train_smote, y_train_smote),\n",
    "    'undersampled': (X_train_under, y_train_under)\n",
    "}\n",
    "\n",
    "print(f\"\\nTraining sets prepared for model evaluation:\")\n",
    "for name, (X_set, y_set) in training_sets.items():\n",
    "    print(f\"- {name}: {X_set.shape[0]} samples, fraud rate: {y_set.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd527ac",
   "metadata": {},
   "source": [
    "<a id=\"models\"></a>\n",
    "\n",
    "## 4. Model Implementation and Evaluation\n",
    "\n",
    "This section implements and evaluates multiple machine learning approaches:\n",
    "\n",
    "### 4.1 Random Forest Classifier\n",
    "\n",
    "- Ensemble method particularly effective for fraud detection\n",
    "- Handles imbalanced data well with class weighting\n",
    "- Provides feature importance insights\n",
    "\n",
    "### 4.2 XGBoost Classifier\n",
    "\n",
    "- Gradient boosting algorithm optimized for structured data\n",
    "- Excellent performance on imbalanced datasets\n",
    "- Built-in regularization and early stopping\n",
    "\n",
    "### 4.3 Support Vector Machine\n",
    "\n",
    "- Effective for high-dimensional data\n",
    "- Can capture complex non-linear patterns with RBF kernel\n",
    "- Good generalization capabilities\n",
    "\n",
    "### 4.4 Neural Network\n",
    "\n",
    "- Deep learning approach with multiple hidden layers\n",
    "- Batch normalization and dropout for regularization\n",
    "- Suitable for large-scale fraud detection systems\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
